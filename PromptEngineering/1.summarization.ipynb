{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff58632e-a957-47d6-ab59-623e3e867b56",
   "metadata": {},
   "source": [
    "# 베드락을 이용하여 리뷰, 상담내역 요약 및 분석 요약하기\n",
    "\n",
    "> *이 노트북은 SageMaker Studio*의 `JupyterLab` 에서 테스트하였습니다. \n",
    "\n",
    "## 소개\n",
    "\n",
    "이 노트북에서는 리테일 고객이 크기가 큰 문서를 요약하는 방법을 보여드리겠습니다.대용량 문서로 작업할 때 입력 텍스트가 모델 컨텍스트 길이에 맞지 않거나, 모델이 대용량 문서를 인식하지 못하거나, 메모리 부족 오류 등으로 인해 몇 가지 문제에 직면할 수 있습니다. 이러한 문제를 해결하기 위해 청킹 및 연쇄 프롬프트 개념을 기반으로 하는 아키텍처를 보여드리겠습니다. 이 아키텍처는 언어 모델로 구동되는 애플리케이션을 개발하는 데 널리 사용되는 프레임워크인 LangChain을 활용합니다.\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html)은 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크입니다. 이 프레임워크의 핵심 측면을 통해 다양한 구성 요소를 연결하여 고급 사용 사례를 만들어 대규모 언어 모델을 보강할 수 있습니다.\n",
    "\n",
    "\n",
    "#### 컨텍스트\n",
    "\n",
    "이 노트북에서는 LangChain 프레임워크 내에서 Amazon Bedrock과 통합하여 사용는 방법과 PromptTemplate의 도움으로 텍스트를 생성하는 데 어떻게 사용될 수 있는지 살펴보겠습니다.\n",
    "\n",
    "\n",
    "#### 사용사례\n",
    "\n",
    "이 접근 방식은 통화 녹취록, 회의 녹취록, 책, 기사, 블로그 게시물 및 상품/서비스 관련 콘텐츠를 요약하는 데 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "#### 구현 방법\n",
    "\n",
    "이 사용 사례를 보여주기 위해 이 노트북에서는 고객의 이전 제품 설명을 기반으로 신규 제품 설명을 생성하는 방법을 보여드리며, Boto3 클라이언트와 함께 Amazon Bedrock API를 사용하는 Anthropic Claude 모델을 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871168af-c682-40c9-94c3-f98a9fe2f358",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "이 노트북의 나머지 부분을 실행하기 전에 아래 셀을 실행하여 (필요한 라이브러리가 설치되어 있는지 확인하고) 베드락에 연결해야 합니다.\n",
    "\n",
    "우선 사전에 설치가 필요한 패키지들을 설치하세요. 그 이후에 셋업에 필요한 라이브러리들을 설치합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bcd67-4dd5-4a7a-8171-78f461a63308",
   "metadata": {},
   "source": [
    "#### 앞 부분은 이전 실습 과정에서 했던 내용과 동일합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8080c-81fa-4b77-8917-c8354c52fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r dependencies/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca881a-b00a-4891-82f6-ecf9f98bc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# this is setting the maximum number of times boto3 will attempt our call to bedrock\n",
    "my_region = \"us-east-1\" # change this value to point to a different region\n",
    "my_config = Config(\n",
    "    region_name = my_region,\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 3,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "# this creates our client we will use to access Bedrock\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", config = my_config)\n",
    "bedrock = boto3.client(\"bedrock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e024cf7-315c-4cc5-81b5-ad3583e69871",
   "metadata": {},
   "source": [
    "## 베드락 LLM 모델 호출하기\n",
    "\n",
    "LLM에서 Bedrock 클래스의 인스턴스를 생성하는 것으로 시작하겠습니다. 여기에는 Amazon Bedrock에서 사용할 수 있는 모델의 ARN인 model_id가 필요합니다.\n",
    "\n",
    "선택적으로 이전에 생성한 boto3 클라이언트를 전달할 수 있으며, `temperature`, `top_p`, `max_tokens` 또는 `stop_sequences`와 같은 매개 변수를 보유할 수 있는 일부 `model_kwargs`도 전달할 수 있습니다(매개 변수에 대한 자세한 내용은 Amazon Bedrock 콘솔에서 살펴볼 수 있습니다).\n",
    "\n",
    "Amazon Bedrock에서 사용 가능한 텍스트 생성 모델 ID에 대한 [설명서](https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/model-ids-arns.html)를 확인하세요.\n",
    "\n",
    "모델마다 지원하는 `model_kwargs`가 다르다는 점에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22f401-d2c1-4a12-a721-1e52f30b4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# to switch to claude v3 Haiku you can use this id\n",
    "haiku_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"Human\"],\n",
    "}\n",
    "\n",
    "# this defines a sonnet object\n",
    "llm = BedrockChat(\n",
    "    client=bedrock_rt,\n",
    "    model_id=sonnet_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "# for any of our chains we can change out the model by simply substituting the sonnet model object with this haiku object.\n",
    "haiku_model = BedrockChat(\n",
    "    client=bedrock_rt,\n",
    "    model_id=haiku_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d3cb3-0ec3-4398-b3d0-a23de3c0f0f2",
   "metadata": {},
   "source": [
    "## 리뷰 텍스트 요약하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df05ab7-ab73-49e8-8611-9390c78e7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Invoke Example, request is a variable we must fill in when we invoke our chain\n",
    "messages = [\n",
    "    (\"user\", \"{request}\"),\n",
    "]\n",
    "\n",
    "# pass our list of messages to our ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# use | to chain together multiple components\n",
    "chain = prompt | llm \n",
    "\n",
    "# request \n",
    "request = \"\"\"\n",
    "\n",
    "Human: 다음 텍스트를 3줄로 요약하세요.\n",
    "<text>\n",
    "1. 핏이 좋고 편안합니다.하지만 세탁 후 색이 번졌어요.그리고 권장대로 세탁했어요.\n",
    "이제 사방에 커다란 분홍색 얼룩이 생겼어요.\n",
    "2. 너무 편안하고 귀엽고 스타일리시합니다.마음에 들어요!\n",
    "3. 이 스웨터가 마음에 들어요.소재는 훌륭하지만 조금 짧았습니다.\n",
    "4. 분명히 위험하다는 건 알았지만 다른 사람들에게서 봤을 때 위글 공간이 조금 더 있을지도 모른다고 생각했지만 XL이 생겼고 예상대로 더 오버사이즈였으면 좋겠어요.\n",
    "그러니 덩치가 큰 제 딸들에게는 생각처럼 오버사이즈가 아니라는 걸 명심하세요!아직 엄청 귀엽고 부드러워요. 하지만 이거 자연 건조할게요!!!\n",
    "5. 마음에 들어요, 멋진 오버사이즈 핏, 귀여운 색상!!\n",
    "</text>\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"request\": request})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779a428-f494-4750-aa2c-f8d7d00aedfc",
   "metadata": {},
   "source": [
    "## 리뷰 텍스트 요약하기\n",
    "\n",
    "PDF 문서를 로드한 후에, 긴 문서를 청킹(chunking)합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666fd84-72b2-4f00-bc6d-8385981a9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/2022-Shareholder-Letter-ko.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a62af-2359-4e55-9c3f-02e15190cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], chunk_size=4000, chunk_overlap=100 \n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553f53e-4715-404a-93fb-988552a2d823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68782d7a-aa51-4e0e-b74d-104a9d465397",
   "metadata": {},
   "source": [
    "## Summarizing chunks and combining them\n",
    "\n",
    "다른 문서에서 토큰의 수가 일정하다고 가정하면 문제가 없을 것입니다. LangChain의 load_summarize_chain을 사용하여 텍스트를 요약해 보겠습니다. load_summarize_chain은 stuff, map_reduce, refine의 세 가지 요약 방법을 제공합니다.\n",
    "\n",
    "* stuff는 모든 청크를 하나의 프롬프트에 넣습니다. 따라서 토큰의 최대 한도에 도달하게 됩니다.\n",
    "* map_reduce는 각 청크를 요약하고, 요약을 결합한 다음, 결합된 요약을 요약합니다. 결합된 요약이 너무 크면 오류가 발생할 수 있습니다.\n",
    "* refine은 첫 번째 청크를 요약한 다음 첫 번째 요약으로 두 번째 청크를 요약합니다. 모든 청크가 요약될 때까지 동일한 프로세스가 반복됩니다.\n",
    "\n",
    "map_reduce와 refine은 LLM을 여러 번 호출하므로 최종 요약을 얻는 데 시간이 걸립니다. 여기에서는 stuff는를 사용해 보겠습니다.\n",
    "\n",
    "'map_reduce' 체인은 큰 문서를 관리하기 쉬운 작은 덩어리로 분할하여 문서 처리를 처리하도록 설계되었습니다. 이 체인은 각 조각에 초기 프롬프트를 사용하여 문서의 특정 섹션을 기반으로 요약 또는 답변을 생성합니다. 또한 MapReduceDocumentsChain은 생성된 출력을 가져와 다른 프롬프트를 사용하여 결합하여 전체 문서에 대한 포괄적이고 일관된 요약 또는 답변을 생성합니다. load_summarize_chain 함수를 사용하여 'map_reduce' 체인을 설정하고 출력 요약을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee01c83-fcca-4121-a824-3c75a331a354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048db2a5-da6b-40d3-9763-9da49d27ae1a",
   "metadata": {},
   "source": [
    "## 요약 결과 분석하기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3416fd1-dd8e-4eed-9219-8dfe236992eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb515071-3a9e-4963-852e-f19e772f1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = f\"\"\" 아래 text의 내용이 (1) 긍정 (2) 부정 (3) 중립 중에 어디에 속하는지 알려주세요. 다른 설명은 하지 말고 바로 긍정, 부정, 중립 이 단어로만 답하세요.:\n",
    "\"{output_summary}\"\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt).content #프롬프트에 응답을 반환\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a5855-d1c9-422f-ad2d-17dbce5c33ec",
   "metadata": {},
   "source": [
    "## Streamlit 어플리케이션 수행하기\n",
    "\n",
    "어플리케이션 수행을 위한 스크립트 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34854d6d-00f4-4e80-bf3f-5d0aa46c880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../summarization_lib.py\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    \n",
    "    model_kwargs =  { #Anthropic 모델\n",
    "        \"max_tokens\": 4000, \n",
    "        \"temperature\": 0\n",
    "        }\n",
    "    \n",
    "    llm = BedrockChat(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", #파운데이션 모델 설정하기\n",
    "        model_kwargs=model_kwargs) #Claude의 속성을 구성합니다.\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "pdf_path = \"PromptEngineering/data/2022-Shareholder-Letter-ko.pdf\"\n",
    "\n",
    "#pdf_path = \"2022-Shareholder-Letter.pdf\"\n",
    "\n",
    "def get_docs():\n",
    "    \n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], chunk_size=4000, chunk_overlap=100 \n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "def get_summary():\n",
    "          \n",
    "    llm = get_llm()\n",
    "    docs = get_docs()\n",
    "    \n",
    "    chain = load_summarize_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "\n",
    "    return chain.invoke(docs, return_only_outputs=True)\n",
    "\n",
    "\n",
    "def get_analysis():\n",
    "\n",
    "    llm = get_llm()\n",
    "    summary = get_summary()\n",
    "    \n",
    "    input_content= f\"\"\" 아래 text의 내용이 (1) 긍정 (2) 부정 (3) 중립 중에 어디에 속하는지 알려주세요. 다른 설명은 하지 말고 바로 긍정, 부정, 중립 이 단어로만 답하세요.:\n",
    "        \"{summary}\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(input_content).content\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae94f8-a746-4091-aec5-5e9b7c937dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../summarization_app.py\n",
    "import streamlit as st\n",
    "import summarization_lib as glib\n",
    "\n",
    "st.set_page_config(layout=\"wide\", page_title=\"문서 요약\")\n",
    "st.title(\"문서 요약 및 감정 분석\")\n",
    "\n",
    "#return_intermediate_steps = st.checkbox(\"중간 단계 반환\", value=True)\n",
    "summarize_button = st.button(\"요약 및 분석\", type=\"primary\")\n",
    "\n",
    "if summarize_button:\n",
    "    #st.subheader(\"요약 및 분석\")\n",
    "\n",
    "    with st.spinner(\"Running...\"):\n",
    "        response_content = glib.get_summary()\n",
    "        analysis_content = glib.get_analysis()\n",
    "\n",
    "        st.write(response_content[\"output_text\"])\n",
    "        st.write(analysis_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f9340-316f-4bdd-924b-bb248226793c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
