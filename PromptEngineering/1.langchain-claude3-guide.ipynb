{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4412f7-f0b4-4ab7-a9f8-a115eb650db8",
   "metadata": {},
   "source": [
    "# Langchain을 사용한 Anthropic Claude3 Prompt Engineering 가이드\n",
    "> *이 노트북은 SageMaker Studio*의 `JupyterLab` 에서 테스트하였습니다. \n",
    "\n",
    "Anthropic Claude Prompt Engineering에 오신 것을 환영합니다! [Anthropic](https://www.anthropic.com/)은 선도적인 [대규모 언어 모델](https://en.wikipedia.org/wiki/Large_language_model)(LLM) 제공업체입니다. Claude 모델 제품군은 Anthropic의 가장 강력한 모델이며, 이 모델의 v3 버전은 많은 [일반적으로 사용되는 벤치마크](https://www.anthropic.com/news/claude-3-family)에서 최상위권에 있습니다.\n",
    "\n",
    "이 노트북에서는 아마존의 대표적인 생성형 AI 서비스인 [Amazon Bedrock](https://aws.amazon.com/bedrock/)과 [LangChain](https://python.langchain.com/docs/get_started/introduction)을 통해 Anthropic의 Claude LLM으로 Prompt Engineering을 수행하는 방법을 안내합니다. 이 가이드는 다양하고 유용한 기술을 소개하지만, 모든 것을 다루지는 않습니다. [여기](https://docs.anthropic.com/claude/docs/claude-2p1-guide)에서 찾을 수 있는 Anthropic의 내부 문서에서 추가적인 팁과 요령을 확인할 수 있습니다.\n",
    "\n",
    "이 노트북은 [LangChain Expression Language](https://python.langchain.com/docs/expression_language/get_started/)의 개념을 사용합니다. \n",
    "\n",
    "이 노트북을 사용하려면 모델 액세스를 통해 Amazon Bedrock에서 Anthropic Claude v3 Sonnet을 활성화해야 합니다. 당분간(2024년 3월 기준) us-west-2(오리건) 또는 us-east-11(버지니아) 지역에서만 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8631c4b-ace9-4fca-aff5-8f3177b561fd",
   "metadata": {},
   "source": [
    "## 의존성 설치\n",
    "\n",
    "시작하기 전에 몇 가지 라이브러리를 설치해야 하는데, 아래 pip 설치 호출을 사용하여 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c2e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -r dependencies/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512a02b-4a37-4ccc-9938-e4431bbcf42d",
   "metadata": {},
   "source": [
    "라이브러리를 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5647d-f981-4ca8-b34e-2a1ae5dbe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import datetime\n",
    "import numpy as np\n",
    "from botocore.config import Config\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "# this is setting the maximum number of times boto3 will attempt our call to bedrock\n",
    "my_region = \"us-west-2\" # change this value to point to a different region\n",
    "my_config = Config(\n",
    "    region_name = my_region,\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 3,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "# this creates our client we will use to access Bedrock\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", config = my_config)\n",
    "bedrock = boto3.client(\"bedrock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ead309-c5a9-4bfd-9727-12eeb2e72e8e",
   "metadata": {},
   "source": [
    "## 기본 사항\n",
    "\n",
    "Claude 모델 제품군은 일반화(generalization)를 잘하는 매우 강력한 모델입니다. 대부분의 경우 일반적이고 덜 체계화된 명령어를 처리할 수 있습니다. 즉, LLM은 훈련 중에 본 패턴을 프롬프트에 넣을 때 결과값이 더 잘 나오는 경우가 많습니다. Claude 모델군도 예외는 아닙니다.\n",
    "\n",
    "Anthropic Claude v2의 경우, 이전에는 프롬프트가 \\n\\nHuman: 으로 시작하고 \\n\\nAssistant: 로 끝났습니다. 그러나 Anthropic Claude 3의 새로운 프롬프트 방식은 [메시지 API](https://docs.anthropic.com/claude/docs/upgrading-from-the-text-completions-api)를 사용합니다. \n",
    "\n",
    "이전에는 Langchain에서 `Bedrock` LLM 객체를 사용하여 이 작업을 수행했습니다. 이전 버전과의 호환성이 개발되었지만, 현재 권장되는 지침은 [BedrockChat](https://python.langchain.com/docs/integrations/chat/bedrock) 객체를 사용하는 것입니다. 이는 기본적으로 메시지 API를 지원합니다. boto3에서와 동일한 모델 파라미터를 model_kwargs로 전달하여 정의할 수 있습니다(익숙하지 않은 분들을 위해 kwarg는 키워드 인수를 의미합니다).\n",
    "\n",
    "여기서는 Sonnet와 Haiku 모두 정의하겠습니다. 이러한 객체를 정의하면 생성하는 모든 체인에서 상호 교환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c0715-642c-4be1-a078-2754936a87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are defining Claude 3 here but the messages API works with older versions of Claude as well\n",
    "sonnet_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# to switch to claude v3 Haiku you can use this id\n",
    "haiku_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"Human\"],\n",
    "}\n",
    "\n",
    "# this defines a sonnet object\n",
    "model = BedrockChat(\n",
    "    client=bedrock_rt,\n",
    "    model_id=sonnet_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "# for any of our chains we can change out the model by simply substituting the sonnet model object with this haiku object.\n",
    "haiku_model = BedrockChat(\n",
    "    client=bedrock_rt,\n",
    "    model_id=haiku_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc804496-6b83-48de-8b5f-af80647ebad6",
   "metadata": {},
   "source": [
    "## Messages API\n",
    "\n",
    "메시지 API를 사용해 보겠습니다. Messages API는 user message, assistant message 또는 text message로 정의할 수 있는 메시지 목록을 받는 방식으로 작동합니다.\n",
    "\n",
    "`ChatPromptTemplate` 객체를 사용하여 `BedrockChat` LLM 객체에 프롬프트를 전달합니다. 괄호 안에 넣는 모든 것은 프롬프트에서 변수로 간주됩니다.\n",
    "\n",
    "그런 다음 파이프(예: `|` 문자)를 사용하여 이러한 객체를 서로 연결할 수 있습니다. [Langchain Express Langchain(LCEL)](https://python.langchain.com/docs/expression_language/why#lcel)을 사용하면 랭체인 요소를 하나의 체인으로 연결하여 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738fc67-4f24-4628-9a61-b972a93e11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Example, question is a variable we must fill in when we invoke our chain\n",
    "messages = [\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "# pass our list of messages to our ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# use | to chain together multiple components\n",
    "chain = prompt | model \n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529720e-79ff-4409-baeb-e32bc7e96b17",
   "metadata": {},
   "source": [
    "### 체인은 실제로 어떻게 생겼나요?\n",
    "\n",
    "체인의 내용을 print하면, 메시지를 포함한 `ChatPromptTemplate`을 구성하는 내용을 확인할 수 있습니다. 이제 `HumanMessagePromptTemplate`로 변환됩니다. 모델 객체에는 `model_kwargs`도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8de17-1582-466f-a05b-9f4edf915497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at chain contents, we can see our {question} variable was picked up by our ChatPromptTemplate\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853c3ab-bee0-4708-b36c-f214437919df",
   "metadata": {},
   "source": [
    "## 출력 파서 (Output Parsers)\n",
    "\n",
    "문자열(text) 출력을 얻으려면 `StrOutputParser` 객체를 전달하면 됩니다. (참고: `StrOutputParser`를 제공하기 이전에는 `AIMessage` 출력형식이었습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96850ca6-e85c-498a-be2c-3ed292470b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# use | to chain together multiple components\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207abcdd-98e5-4052-8120-39d32ffa64f9",
   "metadata": {},
   "source": [
    "## 모델 변경하기\n",
    "\n",
    "Claude Haiku로 바꾸려면 체인에서 모델 오브젝트를 변경하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b40ba8-8cca-4dc3-8ae6-1508d345ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# to change the model simply substitute the model object\n",
    "chain = prompt | haiku_model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c446a8-0acd-4a30-9c17-8d8ec1f6ef4f",
   "metadata": {},
   "source": [
    "## 스트리밍 및 구성 가능한 필드를 추가해 봅시다!\n",
    "\n",
    "기본 `BedrockChat` 객체를 설정했지만 여러 가지 방법으로 사용자 정의할 수 있습니다. 먼저 객체에 스트리밍을 추가해 보겠습니다. 간단한 콜백 클래스를 추가하면 매우 쉽게 할 수 있습니다.\n",
    "`StreamingStdOutCallbackHandler` 콜백을 추가하면 이제 짠! 스트리밍이 생겼습니다! \n",
    "\n",
    "또 다른 멋진 방법은 [런타임에 필드 구성](https://python.langchain.com/docs/expression_language/how_to/configure)을 할 수 있다는 것입니다. 특정 시나리오에서 `max_tokens` 매개변수를 늘리고 싶다고 가정하면, 런타임에 새로운 `model_kwargs`를 허용하도록 `BedrockChat` 객체를 설정할 수 있습니다.\n",
    "\n",
    "아래 블록을 실행하면 응답이 노트북으로 직접 스트리밍되는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6a632-1c00-4aea-bd78-3ed7baa09ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize our BedrockChat object but this time with StreamingStdOutCallbackHandler and configurable model_kwargs\n",
    "model = BedrockChat(\n",
    "    client=bedrock_rt,\n",
    "    model_id=sonnet_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ").configurable_fields(model_kwargs=ConfigurableField(id=\"model_kwargs\"))\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# use | to chain together multiple components\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04e112-c8ff-40d4-beab-3567e1910578",
   "metadata": {},
   "source": [
    "## 런타임에 필드 구성하기\n",
    "\n",
    "런타임에 필드를 구성해 보겠습니다. 이 예제에서는 새로운 stop_sequence를 추가하겠습니다. 여기서는 temperature가 0으로 설정되어 있으므로 이전과 비슷한 결과가 나올 것으로 예측됩니다. 새로운 stop_sequence를 포함시켜 보겠습니다. 생성할 때, 해당 토큰을 사용하려고 하면 차단되는 것을 볼 수 있을 것입니다. 이는 출력 크기를 제한하려는 경우에 유용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4cfd0-7b76-4b03-b01b-730970071778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit our max_tokens and stop_sequences\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    #\"top_p\": 1,\n",
    "    \"stop_sequences\": [\"나비\"],\n",
    "}\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# use | to chain together multiple components\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.with_config(configurable={\"model_kwargs\": model_kwargs}).invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352cf9c1-2ba2-46d3-b27c-29b1004b1720",
   "metadata": {},
   "source": [
    "## System prompts\n",
    "\n",
    "이전에는 Human/Assistant 프레임워크에서 'system' 수준 정보가 프롬프트 자체에 포함되었습니다. 이제 Anthropic Claude v3에서는 사용자 메시지와 별도로 해석되는 명시적인 시스템 메시지를 정의할 수 있습니다. 이를 통해 앤트로픽 클로드가 페르소나를 채택하거나 특정 작업에 대한 지침을 설정할 수 있습니다. 시스템 프롬프트에 대한 Anthropic의 지침[여기](https://docs.anthropic.com/claude/docs/system-prompts)을 참조하세요.\n",
    "\n",
    "한 가지 주의할 점은 시스템 메시지가 메시지 목록에서 맨 앞에 와야 한다는 것입니다. 그렇지 않으면 모델에 프롬프트를 전달할 때 오류가 발생합니다.\n",
    "\n",
    "어시스턴트에게 스토리를 전달하되 Amazon에 초점을 맞춰 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d44e3-28f4-4ca7-824f-aaafbae1c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", \"You are an Amazon robot that answers questions but promotes Amazon whenever possible.\"),\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke, this response will be used later so we are giving it a specific variable name\n",
    "prev_response_joke = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009843b-97e3-4526-8343-85c3f81a6809",
   "metadata": {},
   "source": [
    "\n",
    "## XML 태그\n",
    "\n",
    "Claude는 XML 태그를 이해하도록 훈련되었으므로 이를 통해 프롬프트 구조를 더 잘 이해할 수 있습니다. 몇 가지 구체적인 출력 형식 지침을 통해 이를 시도해 보겠습니다.\n",
    "\n",
    "자세한 내용은 Anthropic의 [문서](https://docs.anthropic.com/claude/docs/use-xml-tags)를 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830969e-9390-4c69-a8c8-b013ea0b96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our system message\n",
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible.</instructions>\n",
    "<output format>Output the joke as the first line, then create a bullet list of the benefits of Amazon Prime membership underneath two lines down.</output_format>\"\"\"\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993726fe-7f26-4e98-93b4-fe681276edfd",
   "metadata": {},
   "source": [
    "## Claude의 입에서 나오는 단어 (Words in Claude's mouth)\n",
    "\n",
    "이전에는 'Assistant' 태그 뒤에 글을 써서 클로드의 입에 단어를 넣을 수 있었습니다. 이제 assistant message를 사용하여 인공 지능 클로드의 입에 단어를 넣을 수 있습니다.\n",
    "\n",
    "인공 지능 클로드의 입에 단어를 넣으려면 이 assistant message가 목록의 마지막 메시지여야 하며, 그렇지 않으면 채팅 기록으로 간주됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e75bb-3c98-4a40-85e6-463211bbfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our system message\n",
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible.</instructions>\"\"\"\n",
    "\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{question}\"),\n",
    "    (\"assistant\", \"I was at my house getting an Amazon package\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b8e3d-264a-430e-a66f-a97873ec8657",
   "metadata": {},
   "source": [
    "## 채팅 내역\n",
    "\n",
    "메시지 목록을 사용하여 이전 상호작용을 전달할 수 있습니다(`ChatMessageHistory`로 이 작업을 수행할 수도 있습니다). 여기서는 Claude가 보낸 이전 응답을 전달하고 새로운 동시 작성을 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc85d4d-48ab-4cba-b582-e0672935817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible.</instructions>\"\"\"\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"봄을 주제로 동시를 쓸 수 있나요?\"),\n",
    "    # this variable is the output of a previous back and forth we had with claude\n",
    "    (\"assistant\", prev_response_joke),\n",
    "    (\"human\", \"{question}\"),\n",
    "\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"여름을 주제로 한 동시는 어때?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12835ae9-a303-44ab-88f4-0149ac20e32c",
   "metadata": {},
   "source": [
    "## JSON으로 포맷하기\n",
    "\n",
    "system message를 사용하여 이전 버전의 Anthropic Claude에서 했던 것과 동일한 유형의 JSON 형식 지정 명령을 수행할 수 있습니다.\n",
    "\n",
    "즉, 다른 출력 파서를 사용할 수도 있습니다. 대신 `JsonOutputParser`를 사용하여 JSON 객체를 출력으로 가져와 보겠습니다. JSON 파서에 대한 자세한 내용은 [여기](https://python.langchain.com/docs/modules/model_io/output_parsers/types/json)를 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773b986-3fe0-452f-9876-60a0e9a9e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible. You also respond only in JSON and Korean.</instructions>\"\"\"\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7c8ae-2fee-48ca-9525-e5076509d33a",
   "metadata": {},
   "source": [
    "## JSON을 사용한 특정 서식 지정\n",
    "\n",
    "이제 동일한 작업을 수행하되 구체적인 출력 서식 지정 지침을 사용해 보겠습니다. Claude에 대해 더 구체적으로 설명할수록 응답이 더 정확하고 일관성이 높아집니다.\n",
    "\n",
    "프롬프트에서 대괄호 사용에 대한 참고 사항입니다. Claude 명령을 JSON 형식으로 표시하려면 LangChain이 입력 변수로 단일 대괄호를 채우려고 시도하므로 별도의 대괄호로 괄호로 묶어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c41b8-ef1c-4f64-916f-d9c95355ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show JSON to Claude\n",
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible. You also respond only in JSON. Look to the output_format to see how to structure your response.</instructions>\n",
    "<output_format>\n",
    "```json\n",
    "{{\"my_poem\":\"poem here\",\n",
    "\"my_promotions\":\"promotions here\"}}\n",
    "```\n",
    "</output_format>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Invoke Example\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "\n",
    "# Chain Invoke\n",
    "response = chain.invoke({\"question\": \"봄을 주제로 동시를 쓸 수 있나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcd7ac-73a9-4607-af5f-66c5465a70f6",
   "metadata": {},
   "source": [
    "## 이미지 해석\n",
    "\n",
    "Claude v3 모델의 주요 이점 중 하나는 이미지를 해석하는 기능입니다. base64로 인코딩된 이미지를 전달하면 이 작업을 수행할 수 있습니다. 이미지를 전달하려면 약간 다른 입력 형식을 사용해야 합니다. 이전에는 메시지 목록을 전달하고 이를 `HumanMessage`, `SystemMessage` 또는 `AIMessage` 객체로 변환하도록 했습니다. 여기서는 입력 이미지를 `HumanMessage`에 직접 전달할 수 있습니다. base64 문자열과 media_type을 전달합니다. 그런 다음 이미지와 함께 텍스트를 전달할 수 있습니다. 시스템 메시지를 `SystemMessagePromptTemplate`을 사용하여 전달할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717430bb-ce2a-4061-8ffb-ab5fe3c82f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an image of an ant we will analyze, load it as base64.\n",
    "with open(\"images/Camponotus_flavomarginatus_ant.jpg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "<instructions>You are an Amazon robot that answers questions but promotes Amazon whenever possible. You also respond only in JSON.</instructions>\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "            system_prompt\n",
    "        ), \n",
    "    HumanMessage(\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"data\": encoded_string,\n",
    "                    \"media_type\":\"image/jpeg\",\n",
    "                    \"type\":\"base64\"\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"이 이미지에는 무엇이 담겨 있나요??\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "image_chain = prompt | model | StrOutputParser()\n",
    "\n",
    "image_out = image_chain.invoke({\"encoded_string\":encoded_string})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c437f-d233-47ab-a8e1-362eda750b8a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is the end of this short introductory notebook, to see more complex methods using LangChain and Claude v3, see the other LangChain notebooks like `advanced_claude_prompting_v3_lcel.ipynb` to learn how to chain calls together and intelligently route them. Happy building!\n",
    "\n",
    "## 결론\n",
    "\n",
    "이 짧은 소개 노트북은 이것으로 끝이며, LangChain과 Claude v3를 사용하는 더 복잡한 방법을 보려면, 다른 LangChain 노트북을 참조하여 호출을 함께 연결하고 지능적으로 라우팅하는 방법을 배우십시오. 행복한 빌딩!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
